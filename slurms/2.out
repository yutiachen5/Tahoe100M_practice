dcc-core-gpu-40
Sun Jun  8 13:36:55 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX 5000 Ada Gene...    On  |   00000000:13:00.0 Off |                    0 |
| 30%   20C    P8             15W /  250W |       2MiB /  30712MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
loaded 330517 observations
number of unique genes: 47278
Transformer(
  (emb_gene): Embedding(47278, 128)
  (proj_expression): Linear(in_features=1, out_features=128, bias=True)
  (proj_gene_input): Linear(in_features=128, out_features=512, bias=True)
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-2): 3 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=512, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (proj_smiles_input): Linear(in_features=768, out_features=512, bias=True)
  (regression_head): Sequential(
    (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (1): Dropout(p=0.1, inplace=False)
    (2): Linear(in_features=512, out_features=256, bias=True)
    (3): ReLU()
    (4): Dropout(p=0.1, inplace=False)
    (5): Linear(in_features=256, out_features=1, bias=True)
  )
)
number of params:  11378177
number of trainable params:  11378177
training...
Epoch [1/100]  Train Loss: 3.0746  Val Loss: 2.7605
Epoch [2/100]  Train Loss: 2.8053  Val Loss: 2.7356
Epoch [3/100]  Train Loss: 2.7735  Val Loss: 2.6871
Epoch [4/100]  Train Loss: 2.4962  Val Loss: 1.9864
Epoch [5/100]  Train Loss: 1.9015  Val Loss: 1.6926
Epoch [6/100]  Train Loss: 1.6822  Val Loss: 1.5767
Epoch [7/100]  Train Loss: 1.6020  Val Loss: 1.5077
Epoch [8/100]  Train Loss: 1.5239  Val Loss: 1.4412
Epoch [9/100]  Train Loss: 1.4690  Val Loss: 1.5119
Epoch [10/100]  Train Loss: 1.4244  Val Loss: 1.3647
Epoch [11/100]  Train Loss: 1.3835  Val Loss: 1.3505
Epoch [12/100]  Train Loss: 1.3223  Val Loss: 1.3524
Epoch [13/100]  Train Loss: 1.2182  Val Loss: 1.1367
Epoch [14/100]  Train Loss: 1.1270  Val Loss: 1.0070
Epoch [15/100]  Train Loss: 1.0240  Val Loss: 0.9972
Epoch [16/100]  Train Loss: 0.9651  Val Loss: 0.9112
Epoch [17/100]  Train Loss: 0.8631  Val Loss: 0.7973
Epoch [18/100]  Train Loss: 0.7936  Val Loss: 0.7006
Epoch [19/100]  Train Loss: 0.7315  Val Loss: 0.6594
Epoch [20/100]  Train Loss: 0.6721  Val Loss: 0.7445
Epoch [21/100]  Train Loss: 0.5956  Val Loss: 0.5940
Epoch [22/100]  Train Loss: 0.5143  Val Loss: 0.4468
Epoch [23/100]  Train Loss: 0.4830  Val Loss: 0.5027
Epoch [24/100]  Train Loss: 0.4436  Val Loss: 0.4472
Epoch [25/100]  Train Loss: 0.4094  Val Loss: 0.3839
Epoch [26/100]  Train Loss: 0.3831  Val Loss: 0.3353
Epoch [27/100]  Train Loss: 0.3549  Val Loss: 0.3207
Epoch [28/100]  Train Loss: 0.3620  Val Loss: 0.3275
Epoch [29/100]  Train Loss: 0.3053  Val Loss: 0.4057
Epoch [30/100]  Train Loss: 0.2866  Val Loss: 0.2979
Epoch [31/100]  Train Loss: 0.2837  Val Loss: 0.2502
Epoch [32/100]  Train Loss: 0.2501  Val Loss: 0.3951
Epoch [33/100]  Train Loss: 0.2904  Val Loss: 0.4246
Epoch [34/100]  Train Loss: 0.2468  Val Loss: 0.2620
Training complete. Best validation loss: 0.2502
predicting...
MSE: 0.2470, PCC: 0.9859, SCC: 0.9860
model saved to /hpc/home/yc583/Tahoe100M_practice/saved_mdls/ic50_predictor.pth
Elapsed time: 1748949410.8 hours
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mrun_s_300[0m at: [34mhttps://wandb.ai/tecchk-cyt-duke-university/Tahoe/runs/ivy626kj[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250608_133709-ivy626kj/logs[0m
