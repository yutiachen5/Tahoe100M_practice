dcc-core-gpu-42
Sun Jun  8 13:36:55 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX 5000 Ada Gene...    On  |   00000000:13:00.0 Off |                    0 |
| 30%   16C    P8             14W /  250W |       2MiB /  30712MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
loaded 330517 observations
number of unique genes: 47278
Transformer(
  (emb_gene): Embedding(47278, 128)
  (proj_expression): Linear(in_features=1, out_features=128, bias=True)
  (proj_gene_input): Linear(in_features=128, out_features=512, bias=True)
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-2): 3 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=512, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=512, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (proj_smiles_input): Linear(in_features=768, out_features=512, bias=True)
  (regression_head): Sequential(
    (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (1): Dropout(p=0.1, inplace=False)
    (2): Linear(in_features=512, out_features=256, bias=True)
    (3): ReLU()
    (4): Dropout(p=0.1, inplace=False)
    (5): Linear(in_features=256, out_features=1, bias=True)
  )
)
number of params:  11378177
number of trainable params:  11378177
training...
Epoch [1/100]  Train Loss: 3.0302  Val Loss: 2.7824
Epoch [2/100]  Train Loss: 2.8079  Val Loss: 2.7542
Epoch [3/100]  Train Loss: 2.7700  Val Loss: 2.9819
Epoch [4/100]  Train Loss: 2.4197  Val Loss: 1.9475
Epoch [5/100]  Train Loss: 1.9001  Val Loss: 1.7057
Epoch [6/100]  Train Loss: 1.7085  Val Loss: 1.5542
Epoch [7/100]  Train Loss: 1.6159  Val Loss: 1.5697
Epoch [8/100]  Train Loss: 1.5475  Val Loss: 1.4679
Epoch [9/100]  Train Loss: 1.4830  Val Loss: 1.5961
Epoch [10/100]  Train Loss: 1.4444  Val Loss: 1.3902
Epoch [11/100]  Train Loss: 1.3975  Val Loss: 1.3474
Epoch [12/100]  Train Loss: 1.3306  Val Loss: 1.3434
Epoch [13/100]  Train Loss: 1.2527  Val Loss: 1.1407
Epoch [14/100]  Train Loss: 1.1380  Val Loss: 1.0654
Epoch [15/100]  Train Loss: 1.0353  Val Loss: 0.9160
Epoch [16/100]  Train Loss: 0.9152  Val Loss: 0.8064
Epoch [17/100]  Train Loss: 0.7981  Val Loss: 0.7335
Epoch [18/100]  Train Loss: 0.7133  Val Loss: 0.9194
Epoch [19/100]  Train Loss: 0.6179  Val Loss: 0.5456
Epoch [20/100]  Train Loss: 0.5509  Val Loss: 0.4986
Epoch [21/100]  Train Loss: 0.4978  Val Loss: 0.5276
Epoch [22/100]  Train Loss: 0.4554  Val Loss: 0.3600
Epoch [23/100]  Train Loss: 0.4252  Val Loss: 0.6165
Epoch [24/100]  Train Loss: 0.3823  Val Loss: 0.4377
Epoch [25/100]  Train Loss: 0.3550  Val Loss: 0.3288
Epoch [26/100]  Train Loss: 0.3389  Val Loss: 0.2942
Epoch [27/100]  Train Loss: 0.3131  Val Loss: 0.2928
Epoch [28/100]  Train Loss: 0.2923  Val Loss: 0.2954
Epoch [29/100]  Train Loss: 0.2848  Val Loss: 0.3255
Epoch [30/100]  Train Loss: 0.2657  Val Loss: 0.2764
Epoch [31/100]  Train Loss: 0.2578  Val Loss: 0.2579
Epoch [32/100]  Train Loss: 0.2578  Val Loss: 0.3393
Epoch [33/100]  Train Loss: 0.2358  Val Loss: 0.2290
Epoch [34/100]  Train Loss: 0.2273  Val Loss: 0.3736
Epoch [35/100]  Train Loss: 0.2301  Val Loss: 1.0007
Epoch [36/100]  Train Loss: 0.2101  Val Loss: 0.2412
Training complete. Best validation loss: 0.2290
predicting...
MSE: 0.2270, PCC: 0.9872, SCC: 0.9869
model saved to /hpc/home/yc583/Tahoe100M_practice/saved_mdls/ic50_predictor.pth
Elapsed time: 1748950964.3 hours
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mrun_s_100[0m at: [34mhttps://wandb.ai/tecchk-cyt-duke-university/Tahoe/runs/ss5pa5h9[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250608_133709-ss5pa5h9/logs[0m
